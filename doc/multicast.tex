\chapter{Multicast Routing}
\label{chap:multicast}
This section describes the usage and the internals of multicast
routing implementation in ns.
We first describe the user interface to enable multicast routing,
specify the multicast routing protocol to be used and the
various methods and configuration parameters specific to the
protocols currently supported in ns.
Then we describe in detail the internals and the architecture of the
multicast routing implementation in ns.

\section{Multicast API}
\label{sec:mcast-api}
Multicast routing is enabled in the simulation by setting 
the \code{EnableMcast_} Simulator variable to 1
before any node, link or agent objects are created.
This is so that the subsequently created 
node, link and agent objects are appropriately
be configured during creation to support multicast routing.
For example the link objects are created with interface labels that
are required by some multicast routing protocols, 
node objects are created with the 
appropriate multicast classifier objects
and agent objects are made to point to the 
appropriate classifier at that node.

A multicast routing strategy is the mechanism by which
the multicast distribution tree
is computed in the simulation.
The multicast routing strategy or protocol
to be used is specified through the mrtproto command.
A handle is returned to an object that has 
methods and configuration parameters specific to a
particular multicast routing strategy or protocol.
A null string is returned otherwise.
There are currently 4 multicast routing strategies in ns: Centralized
Multicast, static Dense Mode, dynamic Dense Mode (i.e., adapts to
network changes), Protocol Independent Multicast - Dense Mode.
Currently only Centralized Multicast returns an object that has
methods and configuration parameters.

An example configuration would be:
\begin{program}
	set ns [new Simulator]
	Simulator set EnableMcast_ 1 \; enable multicast routing;
	Node expandaddr  \; expand address space, required if more than 128 nodes;
	set group [Node allocaddr]   \; allocate a mulicast address;
	set node0 [$ns node]         \; create multicast capable nodes;
	set node1 [$ns node]
	Simulator set NumberInterfaces_ 1  \; number interfaces for all links;
	$ns duplex-link $n0 $n1 1.5Mb 10ms DropTail \; create links with interfaces;

	set mproto dynamicDM          \; configure multicast protocol;
	set mrthandle [$ns mrtproto $mproto  \{\}] \; if an empty list is give, all nodes will contain multicast protocol agents;

	set src [new Agent/CBR]        \; create a source agent at node 0;
	$ns attach-agent $node0 $src 
	$src set dst_ $group

	set rcvr [new Agent/LossMonitor]  \; create a receiver agent at node 1;
	$ns attach-agent $node1 $rcvr
	$ns at 0.3 "$node1 join-group $rcvr $group" \; join the group at simulation time 0.3 (sec);
\end{program}

\subsection{Protocol Specific configuration}

\paragraph{Centralized Multicast}
A Rendezvous Point (RP) rooted shared tree is built for a multicast group.
The actual sending of prune, join messages etc. to set up state at the nodes is not simulated.  A centralized computation agent is used to compute the fowarding trees and set up multicast forwarding state, (S,G) at the relevant nodes as new receivers join a group.  Data packets from the senders to a group are unicast to the RP.  Note that data packets from the senders are unicast to the RP even if there are no receivers for the group. 

Note that whenever network dynamics occur or unicast routing changes, \code{compute-mroutes} is called instantly.  This instantaneous re-computation may iccur causality violations during the transient periods.  Thus, please avoid using centralized multicast if the transient behavior is essential to your study.

Available methods:
Setting multicast protocol
\begin{program}
	set mproto CtrMcast
	set mrthandle [$ns mrtproto $mproto \{\}]
\end{program}

Setting nodes to be candidate RPs and Bootstrap routers (BSRs)
\begin{program}
	$mrthandle set_c_rp \{$node0 $node1\}
	$mrthandle set_c_bsr \{$node0:0 $node1:1\} \;list of node:priority;
\end{program}

Getting RP and BSR
\begin{program}
	$mrthandle get_c_rp $node0 $group
	$mrthandle get_c_bsr $node0
\end{program}

Switching to source-specific trees
\begin{program}
	$mrthandle switch-treetype $group
\end{program}

Re-computing all multicast routes
\begin{program}
	$mrthandle compute-mroutes
\end{program}

\paragraph{Static Dense Mode}
The Static Dense Mode protocol is based on DVMRP with the exception
that it does not adapt to network dynamics.  It uses parent-child lists as in DVMRP to reduce the number of links over which the
data packets are broadcast.  Prune messages for a particular group
are sent upstream by nodes in case they do not lead to any group members.
These prune messages instantiate prune state in the appropriate upstream nodes to prevent multicast packets from being forwarded down links
that do not lead to any group members.  The prune state at the nodes times out after a prune timeout value.  The prune timeout is a class variable in DM.

\begin{program}
	DM set PruneTimeout 0.3           \; default 0.5 (sec);
	set mproto DM
	set mrthandle [$ns mrtproto $mproto \{\}]
\end{program}

\paragraph{Dynamic dense mode}
DVMRP-like dense mode protocol that adapts to network changes is simulated.
'Poison-reverse' information (i.e. the information that a particular neighbouring node uses me to reach a particular network) is read from the routing tables of neighbouring nodes in order to adapt to network dynamics
(DVMRP runs its own unicast routing protocol that exchanges this information).  Currently, this 'Poison-reverse' information is triggerred from network dynamics or unicast changes modules and also updated  periodically.  The 'Poison-reverse' information is updated after a report route timeout value. The report route timeout is a class value in dynamicDM. 
dynamicDM is inherited from DM.

\begin{program}
	dynamicDM set ReportRouteTimeout 0.5  \; default 1 (sec);
	set mproto dynamicDM
	set mrthandle [$ns mrtproto $mproto \{\}]
\end{program}

\paragraph{PIM dense mode}
Protocol Independent Multicast Dense Mode is simulated. 
In this case the data packets are broadcast over all the outgoing 
links except the incoming link. Prune messages are sent by nodes to 
remove the branches of the multicast forwarding tree that do not 
lead to any group members. The prune timeout value is 0.5s by
default (see DM OBJECTS section to change the default). pimDM is inherited from DM.

\begin{program}
	set mproto pimDM
	set mrthandle [$ns mrtproto $mproto \{\}]
\end{program}

\subsection{Multicast Behavior Monitor}
McastMonitor modules counts amounts of packets in transit periodically and prints results to the standard output.  Currently, only prune, join, register, and data packets are traced.  

\begin{program}
	set mcastmonitor [$ns McastMonitor]
	$mcastmonitor set period_ 0.02         \; default 0.03 (sec);
	$mcastmonitor trace-topo $src $group
\end{program}

\section{Internals of multicast routing}
\label{sec:mcast-internals}

We first describe the main classes that are used to implement multicast routing and then describe how each multicast routing strategy
or protocol is implemented.

\subsection{The classes}
The main classes in the implementation are
the McastProtoArbiter class and the McastProtocol class that is the base class for the various multicast routing strategy and protocol objects.
In addition some methods and configuration parameters have been defined in the Simulator, Node, Classifier, and Replicator objects for multicast routing.

\paragraph{McastProtoArbiter class}
There must be one McastProtoArbiter object per multicast capable node.  Each  McastProtoArbiter object maintain a list of multicast protocols.  Usually one multicast routing protocol is configured for the entire simulations.  Whenever there is a multicast related action in a node, the node will acess its multicast protcols through this McastProtoArbiter.

Basic functions include \code{join-group}, \code{leave-group}, and \code{upcall}.  When these functions are called, corresponding functions for all multicast protocols will be called as well.

\paragraph{McastProtocol class}
This is the base class for all the multicast protocols.  It contains basic multicast functions, e.g., \proc[]{join-group}, \proc[]{leave-group}, \proc[]{handle-cache-miss}, \proc[]{handle-wrong-iif}, \proc[]{drop}.  These are mostly virtual functions.  Protocol specific actions are defined in the inheritting multicast protocols classes.

\paragraph{Simulator class}
When \code{EnableMcast_} is set, \code{$ns node} will create multicast nodes (see the following paragraph).

\paragraph{Node class}
Node structure for multicast is slightly different.  The node entry is changed to a switch that distinguishs unicast and multicast packets.  Unicast packets are forwarded to the unicast classifier used to be the entry of a regular Node.  Multicast packets are forwarded to a multicast classifier which maintains a list of replicators with index of source address, group address, and incoming interface. Replicators will copy packets and forward them to all outgoing interfaces. See figure illustrating multicast node sturcture.

\code{join-group} upcalls \code{mcastproto_}'s (the McastProtoArbiter) \code{join-group}, adds the receiver agents to local \code{Agents_} list, and adds outgoing interfaces to all replicators for the group.

\code{leave-group} reverses the process in \code{join-group}.  It disables outgoing interfaces to the receiver agents for all replicators of the group, deletes the receiver agents from the local Agents_ list, and upcall \code{leave-group} at McastProtoArbiter.

\code{new-group} is called from multicast classifier when no proper slots are found.  This function simply makes an upcall to the McastProtoArbiter, \code{mcastproto_}.

\code{add-mfc} does the following: creates a new replicator(if not existing), updates \code{replicator_} and \code{repByGroup_} array lists, adds all outgoing interfaces and local agents to the replicator, and calls multicast classifier's \code{add-rep} to create a slot for the replicator.

\paragraph{Classifier class}
\code{Classifier/Multicast/Replicator} is inherited from Classifier/Multicast (See classifier-mcast.cc).  When receiving of a packet, it parses the packet header and tries to find a matching forwarding slot. If no proper slot is found, it upcalls \code{new-group} defined in OTcl so protocol specific actions can be taken to handle the situation.

\code{new-group} is called with C++ multicast classifier cannot find a slot with proper source, group address, and incoming interface.  If simply no slot is found, \code{code} is set to CACHE_MISS.  If improper incoming interface, \code{code} is set to WRONG_IIF.

\code{add-rep} creates a new slot for a new (S,G,iif) replicator.


\paragraph{Replicator class}
\code{Classifier/Replicator/Demuxer} is inherited from \code{Classifier/Replicator} (See replicator.cc).  When receiving a packet, it copies the packet to all its slots (outgoing interfaces). If no active slot is found, the C++ replicator upcalls OTcl \code{drop} which will trigger protocol specific actions to handle the situation.

\code{insert}, \code{disable}, and \code{enable} help inserting a new outgoing slot, disable an active slot, and enable a previously disabled slot.

\code{change-iface} is to change incoming interface index in the multicast classifier for a replicator.

\code{is-active} and \code{exists} are helper functions to check whether there is any active outgoing slot or a particular outgoing slot is active. 

\subsection{Protocol Internals}
\label{sec:mcastproto-internals}

We describe the implementation of the multicast routing protocol agents in this section.

\subsubsection{Centralized Multicast}

\paragraph{CtrMcast}
\label{CtrMcast}
\code{CtrMcast} is inherited from \code{McastProtocol}.  One CtrMcast agent must be created for a node.  This CtrMcast agent handles multicast related commands for its node, e.g., \proc[]{join-group}, \proc[]{leave-group}, \proc[]{handle-cache-miss}, \proc[]{handle-wrong-iif}, \proc[]{drop}.  When multicast forwarding entries need to be updated, it calls a global, unique CtrMcastComp (centralized multicast computation) agent to compute and install the new entries.  CtrMcastComp is described in ~\ref{CtrMcastComp}.

\proc[]{join-group} basically adds the node to a global member list \code{Mlist} which is an instance variable of \code{CtrMcastComp}, and then calls the CtrMcastComp agent, \code{Agent}, to compute the branches towards all possible sources. \proc[]{leave-group} is the reverse of \proc[]{join-group}

\proc[]{handle-cache-miss} is called when no proper forwarding entry is found for a particular packet source.  In case of centralized multicast, it means a new source just starts.  Thus, the new source is added to a global source list, \code{Slist}.  If there are members in \code{Mlist}, the new multicast tree will be computed. In the case that the particular group is in RPT(shared tree) mode, encapsulation agent is created in the source, decapsulation agent is created in the RP, the two agents are connected by unicast, and the (S,G) entry points its outgoing interface to teh encapsulation agent.

\paragraph{CtrMcastComp}
\label{CtrMcastComp}
\proc[]{reset-mroutes} is to reset all mutlicast forwarding entries.

\proc[]{compute-mroutes} is to recompute all multicast forwarding entries.

\proc[]{compute-tree} is to compute a mutlicast tree by breaking the task to compute branches for all group members.

\proc[]{compute-branch} is to compute a multicast tree branch.  It is basically a for loop starting at the group member and finding the next hop towards the source or RP depending on the tree type until merging with any part of the tree. During the process, several new replicators and an outgoing interface will be installed 

\proc[]{prune-branch} is similar to \proc[]{compute-branch} except the outgoing interface is to be disabled, and when outgoing interface list 


\subsubsection{Static Dense Mode}
\proc[]{join-group} sends graft messages upstream if (S,G) does not contain any active outging slots (i.e., no downstream receivers).

\proc[]{leave-group} does not do anything.

\proc[]{handle-cache-miss} creates (S,G) with only the outgoing interfaces to child nodes. Parent-child relationship is computed only once at the start-up.

\proc[]{drop} sends prune messages upstream because when a packet is dropped, there must be no downstream receivers for the (S,G).

\proc[]{recv-prune} resets prune timer if the interface has been pruned previously, else starts prune timer, disables the interface and forwards the prune message if the outgoing interface list becomes empty.

\proc[]{recv-graft} cancels prune timer, enables the pruned interface, and if the outgoing interface list is previously empty, forwards the graft.


\subsubsection{Dynamic Dense Mode}

\proc[]{periodic-check} periodically updates parent-child relationship for the entire topology.

\proc[]{handle-cache-miss} is excaly the same as in static dense mode except that incoming interface must be set properly so packets from wrong incoming interfaces will be dropped.

\subsubsection{PIM Dense Mode}
proc[]{handle-cache-miss} is exactly the same as in dynamic dense mode except that it outgoing interface list is set to all excluding the incoming interface.

proc[]{handle-wrong-iif} double checks if the incoming interface in the forwarding entry is outdated.  If yes, \proc[]{handle-wrong-iif} updates the forwarding entry to the new incoming interface, else simply sends a prune message back toward the wrong incoming interface to stop packets coming down from the wrong path.

\endinput
