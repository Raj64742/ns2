\chapter{Multicast Routing}
\label{chap:multicast}
This section describes the usage and the internals of multicast
routing implementation in ns.
We first describe the user interface to enable multicast routing,
specify the multicast routing protocol to be used and the
various methods and configuration parameters specific to the
protocols currently supported in ns.
Then we describe in detail the internals and the architecture of the
multicast routing implementation in ns.

\section{Multicast API}
\label{sec:mcast-api}
Multicast routing is enabled in the simulation by setting 
the \code{EnableMcast_} Simulator variable to 1
before any node, link or agent objects are created.
This is so that the subsequently created 
node, link and agent objects are appropriately
be configured during creation to support multicast routing.
For example the link objects are created with interface labels that
are required by some multicast routing protocols, 
node objects are created with the 
appropriate multicast classifier objects
and agent objects are made to point to the 
appropriate classifier at that node.

A multicast routing strategy is the mechanism by which the multicast
distribution tree is computed in the simulation.  The multicast
routing strategy or protocol to be used is specified through the
mrtproto command.  A handle is returned to an object that has methods
and configuration parameters specific to a particular multicast
routing strategy or protocol.  A null string is returned otherwise.
There are currently 4 multicast routing strategies in ns: Centralized
Multicast, static Dense Mode, dynamic Dense Mode (i.e., adapts to
network changes), Protocol Independent Multicast - Dense Mode.
Currently only Centralized Multicast returns an object that has
methods and configuration parameters. Subsequent arguments to command
mrtproto specify the nodes that will run the instance of multicast
routing strategy. The default is to run the same multicast routing
protocol on all the nodes in the topology. As an example, the
following commands illustrate the use of mrtproto command.

\begin{program}
	set cmc [$ns mrtproto CtrMcast \{\}]	\;# specify centralized multicast for all nodes, cmc is the handle for multicast protocol object;
	$ns mrtproto DM $n1 $n2 $n3		\;# specify dense mode multicast for nodes n1, n2 and n3;
	$ns mrtproto dynamicDM 			\;#specidy dynamic dense mode;
\end{program}

New/unused multicast address can be allocated using the procedure
\proc[]{allocaddr}. With default configuration ns can provide multicast
support for only 128 nodes. This bound on number of nodes is due to
the addressing scheme. The procedure \proc[]{expandaddr} can be used to
expand the address space if the number of nodes in simulation is more
than 128. \proc[]{allocaddr\{\}} and \proc[]{expandaddr\{\}} are class
procedures in the class Node.


The agents use the instance procedures join-group and leave-group, in
the class Node to join and leave multicast groups. These procedures
take two mandatory arguments. The first agrument identifies the
corresponding agent and second argument specifies the group address.


An example configuration would be:
\begin{program}
	set ns [new Simulator]
	Simulator set EnableMcast_ 1 \; enable multicast routing;
	Node expandaddr  \; expand address space, required if more than 128 nodes;
	set group [Node allocaddr]   \; allocate a mulicast address;
	set node0 [$ns node]         \; create multicast capable nodes;
	set node1 [$ns node]
	Simulator set NumberInterfaces_ 1  \; number interfaces for all links;
	$ns duplex-link $n0 $n1 1.5Mb 10ms DropTail \; create links with interfaces;

	set mproto dynamicDM          \; configure multicast protocol;
	set mrthandle [$ns mrtproto $mproto  \{\}] \; if an empty list is give, all nodes will contain multicast protocol agents;

	set src [new Agent/CBR]        \; create a source agent at node 0;
	$ns attach-agent $node0 $src 
	$src set dst_ $group

	set rcvr [new Agent/LossMonitor]  \; create a receiver agent at node 1;
	$ns attach-agent $node1 $rcvr
	$ns at 0.3 "$node1 join-group $rcvr $group" \; join the group at simulation time 0.3 (sec);
\end{program}

\subsection{Protocol Specific configuration}

\paragraph{Centralized Multicast}
The centralized multicast is a sparse mode implementation of multicast
similar to PIM-SM. A Rendezvous Point (RP) rooted shared tree is built
for a multicast group.  The actual sending of prune, join messages
etc. to set up state at the nodes is not simulated.  A centralized
computation agent is used to compute the fowarding trees and set up
multicast forwarding state, (S,G) at the relevant nodes as new
receivers join a group.  Data packets from the senders to a group are
unicast to the RP.  Note that data packets from the senders are
unicast to the RP even if there are no receivers for the group.

Note that whenever network dynamics occur or unicast routing changes,
\proc[]{compute-mroutes} is called instantly.  This instantaneous
re-computation may result in causality violations during the transient
periods.  Thus, please avoid using centralized multicast if the
transient behavior is essential to your study.

Available methods:
Setting multicast protocol
\begin{program}
	set mproto CtrMcast
	set mrthandle [$ns mrtproto $mproto \{\}]
\end{program}
The command mrtproto{} returns a handle to the multicast protocol
object. This handle can be used for calling appropriate methods for
switching the tree type and computing multicast routes etc.

Setting nodes to be candidate RPs and Bootstrap routers (BSRs)
\begin{program}
	$mrthandle set_c_rp \{$node0 $node1\}
	$mrthandle set_c_bsr \{$node0:0 $node1:1\} \;list of node:priority;
\end{program}

Getting RP and BSR
\begin{program}
	$mrthandle get_c_rp $node0 $group
	$mrthandle get_c_bsr $node0
\end{program}

Switching to source-specific trees
\begin{program}
	$mrthandle switch-treetype $group
\end{program}

Re-computing all multicast routes
\begin{program}
	$mrthandle compute-mroutes
\end{program}

\paragraph{Static Dense Mode}
The Static Dense Mode protocol is based on DVMRP with the exception
that it does not adapt to network dynamics.  It uses parent-child
lists as in DVMRP to reduce the number of links over which the data
packets are broadcast.  Prune messages for a particular group are sent
upstream by nodes in case they do not lead to any group members.
These prune messages instantiate prune state in the appropriate
upstream nodes to prevent multicast packets from being forwarded down
links that do not lead to any group members.  The prune state at the
nodes times out after a prune timeout value.  The prune timeout is a
class variable in DM and its value can be changed as shown below.

\begin{program}
	DM set PruneTimeout 0.3           \; default 0.5 (sec);
	set mproto DM
	set mrthandle [$ns mrtproto $mproto \{\}]
\end{program}

This multicast protocol computes parent-child relationships prior to
the start of simulation and does not respond to the topology
changes. It can only be used for studying group membership dynamics.

\paragraph{Dynamic dense mode}
DVMRP-like dense mode protocol that adapts to network changes is
simulated.  'Poison-reverse' information (i.e. the information that a
particular neighbouring node uses me to reach a particular network) is
read from the routing tables of neighbouring nodes in order to adapt
to network dynamics (DVMRP runs its own unicast routing protocol that
exchanges this information).  Currently, this 'Poison-reverse'
information is triggerred from network dynamics or unicast changes
modules and also updated periodically.  The 'Poison-reverse'
information is updated after a report route timeout value. The report
route timeout is a class value in dynamicDM.  The class dynamicDM is
inherited from the class DM.

\begin{program}
	dynamicDM set ReportRouteTimeout 0.5  \; default 1 (sec);
	set mproto dynamicDM
	set mrthandle [$ns mrtproto $mproto \{\}]
\end{program}

\paragraph{PIM dense mode}
Protocol Independent Multicast Dense Mode is simulated.  In this case
the data packets are broadcast over all the outgoing links except the
incoming link. Prune messages are sent by nodes to remove the branches
of the multicast forwarding tree that do not lead to any group
members. The prune timeout value is 0.5s by default (see DM OBJECTS
section to change the default). The class pimDM is inherited from the
class DM.

\begin{program}
	set mproto pimDM
	set mrthandle [$ns mrtproto $mproto \{\}]
\end{program}

\subsection{Multicast Behavior Monitor Configuration}
McastMonitor modules counts amounts of packets in transit periodically
and prints results to the standard output.  Currently, only prune,
join, register, and data packets are traced.

\begin{program}
	set mcastmonitor [$ns McastMonitor]
	$mcastmonitor set period_ 0.02         \; default 0.03 (sec);
	$mcastmonitor trace-topo $src $group
\end{program}

\section{Internals of multicast routing}
\label{sec:mcast-internals}

We first describe the main classes that are used to implement multicast
routing and then describe how each multicast routing strategy
or protocol is implemented.

\subsection{The classes}
The main classes in the implementation are
the McastProtoArbiter class and the McastProtocol class that is the base
class for the various multicast routing strategy and protocol objects.
In addition some methods and configuration parameters have been defined in
the Simulator, Node, Classifier, and Replicator objects for multicast
routing.

\paragraph{McastProtoArbiter class}
There must be one McastProtoArbiter object per multicast capable node.
Each  McastProtoArbiter object maintain a list of multicast protocols.
Usually one multicast routing protocol is configured for the entire
simulations.  Whenever there is a multicast related action in a node, the
node will acess its multicast protcols through this McastProtoArbiter.

Basic functions include \proc[]{join-group}, \proc[]{leave-group}, and
\proc[]{upcall}.  When these functions are called, corresponding functions
for all multicast protocols will be called as well.

\paragraph{McastProtocol class}
This is the base class for all the multicast protocols.  It contains basic
multicast functions, e.g., \proc[]{join-group}, \proc[]{leave-group},
\proc[]{handle-cache-miss}, \proc[]{handle-wrong-iif}, \proc[]{drop}.
These are mostly virtual functions.  Protocol specific actions are defined
in the inheritting multicast protocols classes.

\paragraph{Simulator class}
When \code{EnableMcast_} is set, \proc[]{node} will create multicast
nodes (see the following paragraph).

\paragraph{Node class}
Node structure for multicast is slightly different.  The node entry is
changed to a switch that distinguishs unicast and multicast packets.
Unicast packets are forwarded to the unicast classifier used to be the
entry of a regular Node.  Multicast packets are forwarded to a multicast
classifier which maintains a list of replicators with index of source
address, group address, and incoming interface. Replicators will copy
packets and forward them to all outgoing interfaces. See figure
illustrating multicast node sturcture.

\proc[]{join-group} upcalls \code{mcastproto_}'s (the McastProtoArbiter)
\proc[]{join-group}, adds the receiver agents to local \code{Agents_} list,
and adds outgoing interfaces to all replicators for the group.

\proc[]{leave-group} reverses the process in \proc[]{join-group}.  It disables
outgoing interfaces to the receiver agents for all replicators of the
group, deletes the receiver agents from the local Agents_ list, and upcall
\proc[]{leave-group} at McastProtoArbiter.

\proc[]{new-group} is called from multicast classifier when no proper slots
are found.  This function simply makes an upcall to the McastProtoArbiter,
\code{mcastproto_}.

\proc[]{add-mfc} does the following: creates a new replicator(if not
existing), updates \code{replicator_} and \code{repByGroup_} array lists,
adds all outgoing interfaces and local agents to the replicator, and calls
multicast classifier's \proc[]{add-rep} to create a slot for the replicator.

\paragraph{Classifier class}
\code{Classifier/Multicast/Replicator} is inherited from
Classifier/Multicast (See classifier-mcast.cc).  When receiving of a
packet, it parses the packet header and tries to find a matching
forwarding slot. If no proper slot is found, it upcalls \proc[]{new-group}
defined in OTcl so protocol specific actions can be taken to handle the
situation.

\proc[]{new-group} is called with C++ multicast classifier cannot find a
slot with proper source, group address, and incoming interface.  If simply
no slot is found, \code{code} is set to CACHE_MISS.  If improper incoming
interface, \code{code} is set to WRONG_IIF.

\proc[]{add-rep} creates a new slot for a new (S,G,iif) replicator.


\paragraph{Replicator class}
\code{Classifier/Replicator/Demuxer} is inherited from
\code{Classifier/Replicator} (See replicator.cc).  When receiving a
packet, it copies the packet to all its slots (outgoing interfaces). If no
active slot is found, the C++ replicator upcalls OTcl \proc[]{drop} which
will trigger protocol specific actions to handle the situation.

\proc[]{insert}, \proc[]{disable}, and \proc[]{enable} help inserting a new
outgoing slot, disable an active slot, and enable a previously disabled
slot.

\proc[]{change-iface} is to change incoming interface index in the multicast
classifier for a replicator.

\proc[]{is-active} and \proc[]{exists} are helper functions to check whether
there is any active outgoing slot or a particular outgoing slot is active. 

\subsection{Protocol Internals}
\label{sec:mcastproto-internals}

We describe the implementation of the multicast routing protocol agents in
this section.

\subsubsection{Centralized Multicast}

\paragraph{CtrMcast}
\label{CtrMcast}
\code{CtrMcast} is inherited from \code{McastProtocol}.  One CtrMcast
agent needs to be created for a node.  This CtrMcast agent handles multicast
commands for its node, e.g., \proc[]{join-group}, 
\proc[]{leave-group}, and \proc[]{handle-cache-miss}.
When multicast forwarding entries need to be updated, it calls a 
global, unique CtrMcastComp
(centralized multicast computation) agent to compute and install the new
entries.  CtrMcastComp is described in ~\ref{CtrMcastComp}.

\proc[]{join-group} basically adds the node to a global member list
\code{Mlist} which is an instance variable of \code{CtrMcastComp}, and
then calls the CtrMcastComp agent, \code{Agent}, to compute the branches
towards all possible sources. \proc[]{leave-group} is the reverse of
\proc[]{join-group}

\proc[]{handle-cache-miss} is called when no proper forwarding entry is
found for a particular packet source.  In case of centralized multicast,
it means a new source just starts.  Thus, the new source is added to a
global source list, \code{Slist}.  If there are members in \code{Mlist},
the new multicast tree will be computed. In the case that the particular
group is in RPT(shared tree) mode, encapsulation agent is created in the
source, decapsulation agent is created in the RP, the two agents are
connected by unicast, and the (S,G) entry points its outgoing interface to
teh encapsulation agent.

\paragraph{CtrMcastComp}
\label{CtrMcastComp}
\proc[]{reset-mroutes} is to reset all mutlicast forwarding entries.

\proc[]{compute-mroutes} is to recompute all multicast forwarding entries.

\proc[]{compute-tree} is to compute a mutlicast tree by breaking the task
to compute branches for all group members.

\proc[]{compute-branch} is to compute a multicast tree branch.  It is
basically a for loop starting at the group member and finding the next hop
towards the source or RP depending on the tree type until merging with any
part of the tree. During the process, several new replicators and an
outgoing interface will be installed 

\proc[]{prune-branch} is similar to \proc[]{compute-branch} except the
outgoing interface is to be disabled, and when outgoing interface list 


\subsubsection{Static Dense Mode}
\proc[]{join-group} sends graft messages upstream if (S,G) does not
contain any active outging slots (i.e., no downstream receivers).

\proc[]{leave-group} does not do anything.

\proc[]{handle-cache-miss} creates (S,G) with only the outgoing interfaces
to child nodes. Parent-child relationship is computed only once at the
start-up.

\proc[]{drop} sends prune messages upstream because when a packet is
dropped, there must be no downstream receivers for the (S,G).

\proc[]{recv-prune} resets prune timer if the interface has been pruned
previously, else starts prune timer, disables the interface and forwards
the prune message if the outgoing interface list becomes empty.

\proc[]{recv-graft} cancels prune timer, enables the pruned interface, and
if the outgoing interface list is previously empty, forwards the graft.


\subsubsection{Dynamic Dense Mode}

\proc[]{periodic-check} periodically updates parent-child relationship for
the entire topology.

\proc[]{handle-cache-miss} is excatly the same as in static dense mode
except that incoming interface must be set properly so packets from wrong
incoming interfaces will be dropped.

\subsubsection{PIM Dense Mode}
proc[]{handle-cache-miss} is exactly the same as in dynamic dense mode
except that it outgoing interface list is set to all excluding the
incoming interface.

proc[]{handle-wrong-iif} double checks if the incoming interface in the
forwarding entry is outdated.  If yes, \proc[]{handle-wrong-iif} updates
the forwarding entry to the new incoming interface, else simply sends a
prune message back toward the wrong incoming interface to stop packets
coming down from the wrong path.

\endinput

